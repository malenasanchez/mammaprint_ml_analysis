# Modelosd e entrenamiento para MammaPrint

**Autor:** Malena Sánchez Domínguez  
**Fecha:** 2025-04-09

Antes de todo elegimos el directorio de trabajo 
```r
setwd("C:/Users/Malena")
```

Lo siguiente es cargar las librerias que vamos a necesitar.
- caret sirve para entrenar los modelos de Machine Learning.
- MLeval ayuda a evaluar y visualizar los modelos facilmente
- pROC está especializado en calcular y graficar curvas ROC.
```r
library(caret)
library(MLeval)
library(pROC)
```

A continuación, cargamos todos los archivos con los que vamos a trabajar, los cuales habíamos guardado anteriormente como archivos .rds.
```r
filteredMat <- readRDS("filteredMat.rds")
clinical <- readRDS("clinical_info_MammaPrint.rds")
riesgo <- readRDS("riesgo_MammaPrint.rds")
```

Ahora, convertimos la matriz filtrada en un data frame para facilitar así­ cu manipulación, ya que los data frames son más flexibles que las matrices.
```r
filteredMat <- as.data.frame(filteredMat)
```

Podemos verificar que los datos se han cargado correctamente y explorar las columnas y filas con la función View().
```r
View(filteredMat)
```

También, podemos explorar el contenido interno de cada objeto con la función str().
```r, eval = FALSE
str(filteredMat)
str(clinical)
str(riesgo)
```

Comprobamos las dimensiones de los datos cargados. En este caso, las dimensiones de 'riesgo' nos salen como NULL, ya que anteriormente lo hemos convertido a factor.
```r
dim(filteredMat)
dim(riesgo)
```
Otra de las verificaciones que se pueden hacer es ver si hay valores faltantes con la función is.na().
```r
sum(is.na(filteredMat))
sum(is.na(clinical))
sum(is.na(riesgo))
```

Revisamos la distribución de la variable respuesta con la función table(), con esta función vemos cuántas veces se repite cada valor en el conjunto de datos.
```r
table(riesgo)
```
Comprobamos que los IDs de las muestras coinciden en todos los archivos. Esto es necesario para no entrenar el modelo con datos incorrectos y el modelo no sabría a qué caracterí­sticas corresponden las respuestas. Además, de esta forma evitamos resultados erróneos.
```r
all(rownames(filteredMat) == rownames(clinical))
all(rownames(filteredMat)== names(riesgo))
```

Creamos una partición de los datos 70-30 para el entrenamiento y la validación. La función createDataPartition(), lo que hace es dividir los datos de manera estratificada, de manera que las clases están equilibradas. Para ello, primero creamos una semilla aleatoria para que siempre se obtenga la misma división de datos.
```r
set.seed(234)
trainIdx <- createDataPartition(riesgo, p = 0.7, list = FALSE, times = 1)
```

Generamos los conjuntos de entrenamiento y validacion, donde 'dataTrain' va a contener el 70% de los datos, mientras que 'dataVal' contiene el 30% restante.
```r
dataTrain <- filteredMat[trainIdx, ]
dataVal <- filteredMat[-trainIdx, ]
```


Generamos los vectores de etiquetas para clasificación para los conjuntos de entrenamiento y de validacion. Convertimos a su vez la variable respuesta a factor con niveles válidos
```r
# Convertimos la variable respuesta a factor con niveles válidos
claseTrain <- factor(riesgo[trainIdx],
                     levels = c("Bajo riesgo", "Alto riesgo"),
                     labels = c("BajoRiesgo", "AltoRiesgo"))

claseVal <- factor(riesgo[-trainIdx],
                   levels = c("Bajo riesgo", "Alto riesgo"),
                   labels = c("BajoRiesgo", "AltoRiesgo"))

```

Comprobamos de nuevo el equilibrio entre ambas clases en las particiones generadas
```r
table(riesgo[trainIdx])
table(riesgo[-trainIdx])
```

Generamos los vectores de respuesta para regresión, es decir, extraemos el valor continuo que vamos a predecir en modelos de regresión
```r
scoreTrain <- clinical$MP_score[trainIdx]
scoreVal <- clinical$MP_score[-trainIdx]
```

```r
preProcessPar <- preProcess(dataTrain, method = c("center", "scale"))
trainProcessed <- predict(preProcessPar, dataTrain)
valProcessed   <- predict(preProcessPar, dataVal)
```


Ahora lo que hacemos es combinar los datos normalizados con la variable objetivo, generando los datasets completos que vamos a usar en los modelos.
```r
trainProcessed_class <- cbind(claseTrain, trainProcessed)
valProcessed_class <- cbind(claseVal, valProcessed)
```

AÃ±adimos la variable de respuesta al dataset procesado para regresión
```r
trainProcessed_reg <- cbind(scoreTrain, trainProcessed)
```

Convertimos la variable de respuesta de clasificación en factor con niveles "BajoRiesgo" y "AltoRiesgo. Esto es muy útil para evitar errores en el entrenamiento del modelo.
```r
trainProcessed_class$claseTrain <- factor(claseTrain,
                                          levels = c("Bajo riesgo", "Alto riesgo"),
                                          labels 
```

Creamos el objeto con la información de control del modelo, con función de selección "best". Antes de eso configuramos la semilla de reproducibilidad
```r
set.seed(234)
  
filtCtrl_rf <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 3,
                              selectionFunction = "best",
                              verboseIter = TRUE,
                              search = "grid",
                              savePredictions = TRUE,
                              classProbs = TRUE)
```

Definimos los valores del hiperparámetro 'mtry' que vamos a probar, es decir, especificamos el número de variables que selecciona en cada split. 
```r
tunegrid_rf <- expand.grid(mtry = c(5, 25, 50, 75, 100))
```

Entrenamos el modelo, en este caso un RF, empleando los parámetros de control establecidos anteriormente en trainControl
```r
RF <- train(claseTrain ~ .,
              data = trainProcessed_class,
              method = "rf",
              trControl = filtCtrl_rf,
              verbose = FALSE,
              metric = "Accuracy",
              tuneGrid = tunegrid_rf,
              returnData = T)
```

Inspeccionamos los elementos del objeto creado con el entrenamiento del modelo
```r
  RF$method
```
```r
RF$modelType
```
```r
RF$metric
```
```r
RF$results
```
```r
RF$bestTune
```
```r
RF$call
```
```r
RF$dots
```
```{r, eval = FALSE}
RF$control
```
```{r, eval = FALSE}
RF$finalModel
```
```r
RF$resample
```
```r
RF$times
```
```r
summary(RF)
```

Para obtener y visualizar la importancia de las características ordenadas hacemos lo siguiente:
```r
vars<-varImp(RF)
varsImp<-vars$importance
imp<-data.frame(Imp = varsImp$Overall, Genes = rownames(varsImp))
imp<-imp[order(imp$Imp,decreasing = T),]
dotPlot(vars)
```

Podemos visualizar en un gráfico el output de entrenamiento del modelo
con una funciÃ³n del propio caret:
```r
plot(RF)
```

Calculamos la curva ROC. Para ello, tenemos que guardar primero las probabilidades y predicciones:
```r
fitCtrl <- trainControl(method = "repeatedcv",
                        number = 10, repeats = 3,
                        selectionFunction = "best",
                        verboseIter = TRUE,
                        search = "grid",
                        savePredictions = TRUE,
                        classProbs = TRUE)
```

Cargamos la libreria correspondiente y guardamos las probabilidads y predicciones
```r
library(MLeval)
evalRF<-evalm(RF)
```
Ploteamos la curva ROC usando ggplot2
```r
evalRF$roc
```

Obtenemos la AUC y otras métricas:
```r
evalRF$stdres
```
Ahora vamos a cargar el paquee pROC, el cual se utiliza para generar y visualizar curvas ROC, permitiéndonos así­ evaluar el rendimiento de un modelo de clasificación binaria.
```r
library(pROC)
```

Seleccionamos un parámetro, por ejemplo
```r
selectedIdx <- RF$pred$mtry == 50
```

Ploteamos la curva ROC correspondiente, asumiendo que "AltoRiesgo", es la clase positiva, pero antes de eso tenemos que crear el objeto ROC, ya que nos aparecí­a un error:
```r
roc_obj <- roc(response = RF$pred$obs[selectedIdx],
               predictor = RF$pred$AltoRiesgo[selectedIdx])
```

```r
plot(roc_obj,
     col = "#2c7fb8",
     print.auc = TRUE,
     main = "Curva ROC para mtry = 50")
```
Empleando el modelo entrenado podemos predecir la clase de los datos de validación
```r
predVal <- predict(RF, newdata = dataVal, type = "prob")
predCall<- predict(RF, newdata = dataVal, type="raw")
names(predCall) <- rownames(predVal)
predcall <- predCall
```

Calculamos la matriz de confusión, pero primero tenemos que hacer que los niveles de ambos objetos coincidan:
```r
claseVal <- factor(claseVal,
                   levels = c("Bajo riesgo", "Alto riesgo"),
                   labels = c("BajoRiesgo", "AltoRiesgo"))

confusionMatrix(data = predCall,
               reference = claseVal,
               positive = "AltoRiesgo",
               mode = "everything")
```
Ahora lo que vamos a hacer es entrenar un nuevo modelo, en este caso GBM (Gradient Boosting Machine). Para ello, no es necesario modificar todo, simplemente creamos un nuevo data frame para este modelo y utilizamos la función train(). Antes de eso definimos un tuneGrid para GBM con los hiperparámetros clave del modelo.
```r
tunegrid_gbm <- expand.grid(n.trees = c(100, 150, 200),
                            interaction.depth = c(1, 3, 5),
                            shrinkage = c(0.01, 0.1),
                            n.minobsinnode = c(10))

set.seed(234)
gbmFit <- train(claseTrain ~ ., 
                data = trainProcessed_class,
                method = "gbm",
                trControl = filtCtrl_rf,
                tuneGrid = tunegrid_gbm,
                metric = "Accuracy",
                verbose = FALSE)
```

Al igual que en el caso anterior inspeccionamos los elementos:
```r
gbmFit$method
```
```r
gbmFit$modelType
```
```r
gbmFit$metric
```
```r
gbmFit$results
```
```r
gbmFit$bestTune
```
```r
gbmFit$call
```
```r
gbmFit$dots
```
```{r, eval = FALSE}
gbmFit$control
```
```r
gbmFit$finalModel
```
```r
gbmFit$resample
```
```r
gbmFit$times
```
```r
summary(gbmFit)
```

Ahora vemos los resultados del modelo GBM
```r
library(gbm)
vars<-varImp(gbmFit)
varsImp<-vars$importance
imp<-data.frame(Imp = varsImp$Overall, Genes = rownames(varsImp))
imp<-imp[order(imp$Imp,decreasing = T),]
plot(vars, top = 20, main = "Importancia de Variables - GBM")
```
```r
plot(gbmFit)
```

Luego calculamos y visualizamos las curvas ROC:
```r
evalgbmFit<-evalm(gbmFit)
```
```r
evalgbmFit$roc
```
```r
evalgbmFit$stdres
```

Para generar y visualizar las curvas ROC para GBM, primero seleccionamos el mejor número de árboles (n.trees)
```r
bestTrees <- gbmFit$bestTune$n.trees
selectedIdx <- gbmFit$pred$n.trees == bestTrees
```

Verificamos que hay observaciones de ambas clases
```r
table(gbmFit$pred$obs[selectedIdx])
```

```r
rocgbm <- roc(response = gbmFit$pred$obs[selectedIdx],
              predictor = gbmFit$pred$AltoRiesgo[selectedIdx],
              levels = c("BajoRiesgo", "AltoRiesgo"))

plot(rocgbm, col = "#e6550d", print.auc = TRUE, main = "Curva ROC - GBM")
```
Matriz de confusion: Primero hacemos predicciones de clase en el conjunto de validación
```r
predCall_gbm <- predict(gbmFit, newdata = valProcessed_class)

```

Verificamos los niveles de las clases
```r
levels(predCall_gbm)
levels(claseVal)
```

Creamos la matriz de confusión
```r
confusionMatrix(data = predCall_gbm,
                reference = claseVal,
                positive = "AltoRiesgo",
                mode = "everything")
```
Por último, vamos a entrenar un modelo lineal (lm). En este caso, trabajamos con regresión, en vez de clasificación. En este caso, creamos un nuevo fitCtrl, ya que los anteriores llevaban 'classProb = TRUE',y los modelos lm no utilizan clasificación, sino que predicen valores continuos.

Además, en este caso, tampoco es necesario definir un tuneGrid porque no hay hiperparámetros que ajustar. El modelo de regresión lineal clásica, lo que hace es estimar los coeficientes por mí­nimos cuadrados ordinarios (OLS).

Aplicamos un PCA (preProcess = "pca") para reducir la dimensionalidad, dado que nos aparecia un aviso de "prediction from rank-deficient fit. Este aviso ocurre porque hay multicolinealidad en los datos, por lo que el modelo no estima correctamente los coeficientes. 
```r
fitCtrl_lm <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 3,
                           selectionFunction = "best",
                           verboseIter = TRUE,
                           savePredictions = TRUE) 

set.seed(234)
lmFit <- train(scoreTrain ~ ., 
               data = trainProcessed_reg, 
               method = "lm", 
               trControl = fitCtrl_lm,
               metric = "RMSE",
               preProcess = "pca")


```
Inspeccionamos los elementos del modelo lineal:
```r
lmFit$method
```
```r
lmFit$modelType
```
```r
lmFit$metric
```
```r
lmFit$results
```
```r
lmFit$bestTune
```
```r
lmFit$call
```
```r
lmFit$dots
```
```{r, eval = FALSE}
lmFit$control
```
```{r, eval = FALSE}
lmFit$finalModel
```
```r
lmFit$resample
```
```r
lmFit$times
```
```r
summary(lmFit)
```

```r
summary(lmFit)$r.squared
```

```r
pred_lm <- predict(lmFit, newdata = valProcessed)
rmse <- sqrt(mean((pred_lm - scoreVal)^2))
rmse
```

```r
mae <- mean(abs(pred_lm - scoreVal))
mae
```

```r
mse <- mean((pred_lm - scoreVal)^2)
mse
```
Los resultados los podemos ver de la siguiente forma
```r
vars<-varImp(lmFit)
varsImp<-vars$importance
imp<-data.frame(Imp = varsImp$Overall, Genes = rownames(varsImp))
imp<-imp[order(imp$Imp,decreasing = T),]
dotPlot(vars)
```
En este caso no podemos hacer plot() dado que, el modelo lineal no tiene heperparámetros que ajustar. En su lugar, se puede hacer lo siguiente
```r
summary(lmFit$finalModel)
```

```r
plot(lmFit$finalModel$fitted.values, residuals(lmFit$finalModel),
     xlab = "Valores ajustados",
     ylab = "Residuales",
     main = "Residuals vs Fitted")
abline(h = 0, col = "red", lty = 2)
```

```r
qqnorm(residuals(lmFit$finalModel))
qqline(residuals(lmFit$finalModel), col = "blue")
```
Como lm lo hemos usado para una predicción de valor continuo, no podemos hacer la curva ROC, ya que esta solo es para clasificación binaria. Ocurre lo mismo para calcular la matriz de confusón.
